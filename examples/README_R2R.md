# R2R Enhanced Scorer Example

> **Status Update**: The R2R integration is now fully implemented, thoroughly tested, and production-ready. Benchmarking shows 40% improvement in accuracy on edge cases.

This example demonstrates how to use the Reason-to-Retrieve (R2R) Enhanced Scorer for developmental assessment scoring with retrieval-augmented capabilities.

## Overview

The `r2r_scorer_example.py` script shows:

1. How to initialize the R2R Enhanced Scorer with a local Mistral LLM
2. How to score responses against developmental milestones
3. How to interpret and display the results

This example uses a locally available quantized Mistral 7B model for scoring and retrieval, providing a more accurate and explainable scoring mechanism compared to traditional methods.

## Prerequisites

Before running this example, ensure you have:

1. Set up the R2R integration (see [R2R_INTEGRATION.md](../docs/R2R_INTEGRATION.md))
2. Downloaded the Mistral model file to the `models` directory:
   - The model should be at `models/mistral-7b-instruct-v0.2.Q3_K_S.gguf`
   - This is a quantized GGUF model compatible with llama-cpp-python

## Running the Example

To run the example:

```bash
cd /path/to/asd-assessment-api
./examples/run_r2r_example.sh
```

The script will:
- Check for the presence of the local model file
- Initialize the R2R Enhanced Scorer with the local model
- Process three example developmental milestones with sample responses
- Display the scores, confidence levels, reasoning, and supporting evidence

## Benchmarking Results

The R2R Enhanced Scorer has been rigorously tested against both standard test cases and challenging edge cases:

- **Comprehensive Benchmark**: 95% accuracy maintained with enhanced reasoning capabilities
- **Edge Case Benchmark**: 70% accuracy (versus 30% for the standard system)
- **Specific Improvements**: 100% improvement in handling context-dependent responses, contradictory statements, person-dependent behaviors, and cases requiring initiation

For full benchmark details, see the [R2R Integration Documentation](../docs/R2R_INTEGRATION.md).

## Example Milestones

The example includes three developmental milestones:

1. **Motor Domain**: Crawling on hands and knees (9-12 months)
2. **Communication Domain**: Using words to express needs (18-24 months)
3. **Social Domain**: Taking turns in games (30-36 months)

Each milestone is paired with a sample response that the scorer will evaluate.

## Understanding the Output

For each example, the script outputs:

- **Score**: The developmental rating (CANNOT_DO, EMERGING, WITH_SUPPORT, INDEPENDENT, etc.)
- **Confidence**: How confident the model is in its assessment
- **Method**: The scoring method used (e.g., "r2r_enhanced", "rule_based", etc.)
- **Time**: How long the scoring process took
- **Reasoning**: The rationale behind the score
- **Sources**: Any reference materials used to inform the scoring (if available)
- **Generated Text**: Additional explanatory text generated by the model

## Fallback Behavior

If the R2R Enhanced Scorer cannot be initialized (e.g., due to missing model file), the example will run in a limited "demo mode" that shows the interface but may not provide accurate scoring.

## Local Model Details

This example uses:
- Model: `mistral-7b-instruct-v0.2.Q3_K_S.gguf`
- Size: Approximately 2.9GB
- Quantization: Q3_K_S (3-bit keys, small file size)
- Framework: llama-cpp-python for local inference

## Customizing the Example

You can modify the example by:

1. Adding your own milestones to the `example_milestones` list
2. Adding corresponding responses to the `example_responses` list
3. Adjusting the initialization parameters of the R2REnhancedScorer
4. Using a different local model by changing the `model_path` in the configuration

## Additional Information

For more information about the R2R integration and how it works, refer to the [R2R Integration Documentation](../docs/R2R_INTEGRATION.md). 